{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Case Study (1).pdf: 5 pages loaded\n",
      "Processing Case Study (10).pdf: 4 pages loaded\n",
      "Processing Case Study (100).pdf: 6 pages loaded\n",
      "Processing Case Study (101).pdf: 5 pages loaded\n",
      "Processing Case Study (102).pdf: 6 pages loaded\n",
      "Processing Case Study (103).pdf: 5 pages loaded\n",
      "Processing Case Study (104).pdf: 4 pages loaded\n",
      "Processing Case Study (105).pdf: 4 pages loaded\n",
      "Processing Case Study (106).pdf: 6 pages loaded\n",
      "Processing Case Study (107).pdf: 10 pages loaded\n",
      "Processing Case Study (108).pdf: 7 pages loaded\n",
      "Processing Case Study (109).pdf: 7 pages loaded\n",
      "Processing Case Study (11).pdf: 6 pages loaded\n",
      "Processing Case Study (110).pdf: 5 pages loaded\n",
      "Processing Case Study (111).pdf: 6 pages loaded\n",
      "Processing Case Study (112).pdf: 5 pages loaded\n",
      "Processing Case Study (113).pdf: 5 pages loaded\n",
      "Processing Case Study (114).pdf: 4 pages loaded\n",
      "Processing Case Study (115).pdf: 6 pages loaded\n",
      "Processing Case Study (116).pdf: 3 pages loaded\n",
      "Processing Case Study (117).pdf: 12 pages loaded\n",
      "Processing Case Study (118).pdf: 7 pages loaded\n",
      "Processing Case Study (119).pdf: 4 pages loaded\n",
      "Processing Case Study (12).pdf: 11 pages loaded\n",
      "Processing Case Study (120).pdf: 8 pages loaded\n",
      "Processing Case Study (121).pdf: 3 pages loaded\n",
      "Processing Case Study (122).pdf: 7 pages loaded\n",
      "Processing Case Study (123).pdf: 6 pages loaded\n",
      "Processing Case Study (124).pdf: 4 pages loaded\n",
      "Processing Case Study (125).pdf: 5 pages loaded\n",
      "Processing Case Study (126).pdf: 5 pages loaded\n",
      "Processing Case Study (127).pdf: 3 pages loaded\n",
      "Processing Case Study (128).pdf: 7 pages loaded\n",
      "Processing Case Study (129).pdf: 5 pages loaded\n",
      "Processing Case Study (13).pdf: 6 pages loaded\n",
      "Processing Case Study (130).pdf: 8 pages loaded\n",
      "Processing Case Study (131).pdf: 7 pages loaded\n",
      "Processing Case Study (132).pdf: 8 pages loaded\n",
      "Processing Case Study (133).pdf: 5 pages loaded\n",
      "Processing Case Study (134).pdf: 11 pages loaded\n",
      "Processing Case Study (135).pdf: 6 pages loaded\n",
      "Processing Case Study (136).pdf: 7 pages loaded\n",
      "Processing Case Study (137).pdf: 10 pages loaded\n",
      "Processing Case Study (138).pdf: 2 pages loaded\n",
      "Processing Case Study (139).pdf: 6 pages loaded\n",
      "Processing Case Study (14).pdf: 7 pages loaded\n",
      "Processing Case Study (140).pdf: 5 pages loaded\n",
      "Processing Case Study (141).pdf: 6 pages loaded\n",
      "Processing Case Study (142).pdf: 6 pages loaded\n",
      "Processing Case Study (143).pdf: 4 pages loaded\n",
      "Processing Case Study (144).pdf: 2 pages loaded\n",
      "Processing Case Study (145).pdf: 3 pages loaded\n",
      "Processing Case Study (146).pdf: 6 pages loaded\n",
      "Processing Case Study (147).pdf: 3 pages loaded\n",
      "Processing Case Study (148).pdf: 3 pages loaded\n",
      "Processing Case Study (149).pdf: 7 pages loaded\n",
      "Processing Case Study (15).pdf: 4 pages loaded\n",
      "Processing Case Study (150).pdf: 6 pages loaded\n",
      "Processing Case Study (151).pdf: 7 pages loaded\n",
      "Processing Case Study (152).pdf: 7 pages loaded\n",
      "Loaded 348 PDF documents\n",
      "Created 348 text chunks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import json\n",
    "# Set your OpenAI API key\n",
    "# import os#\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "embedding = OpenAIEmbeddings(api_key=openai_key)\n",
    " \n",
    "# Specify your PDF folder path\n",
    "pdf_folder_path = r\"C:\\Users\\surya_chakka\\BD_Assistant\\CaseStudies\"\n",
    " \n",
    "# Load all PDFs from the folder\n",
    "# documents = []\n",
    "# for file in os.listdir(pdf_folder_path):\n",
    "#     if file.endswith('.pdf'):\n",
    "#         pdf_path = os.path.join(pdf_folder_path, file)\n",
    "#         loader = PyMuPDFLoader(pdf_path)\n",
    "#         documents.extend(loader.load())\n",
    "# ... existing imports ...\n",
    " \n",
    "# Load first 2 pages from all PDFs in the folder\n",
    "# ... existing imports ...\n",
    " \n",
    "documents = []\n",
    "for file in os.listdir(pdf_folder_path):\n",
    "    if file.endswith('.pdf'):\n",
    "        try:\n",
    "            pdf_path = os.path.join(pdf_folder_path, file)\n",
    "            loader = PyMuPDFLoader(pdf_path)\n",
    "            # Load all pages\n",
    "            pages = loader.load()\n",
    "            print(f\"Processing {file}: {len(pages)} pages loaded\")\n",
    "            documents.extend(pages)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {str(e)}\")\n",
    "            continue\n",
    " \n",
    "# Add a check before proceeding\n",
    "if not documents:\n",
    "    raise ValueError(\"No documents were successfully loaded\")\n",
    " \n",
    "# ... rest of the code remains same ...\n",
    "# ... rest of the code remains same ...\n",
    " \n",
    "# ... rest of the code remains same ...\n",
    "# Split documents into chunks (optional but recommended)\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    " \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \"]\n",
    ")\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    " \n",
    "# Create vector store\n",
    "db = FAISS.from_documents(split_documents, embedding)\n",
    " \n",
    "# Save the vector store locally (optional)\n",
    "db.save_local(\"./Index\")\n",
    " \n",
    "# Print some stats\n",
    "print(f\"Loaded {len(documents)} PDF documents\")\n",
    "print(f\"Created {len(split_documents)} text chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db=FAISS.load_local(\"./Index\",embedding)    \n",
    "except:\n",
    "    db=FAISS.load_local(\"./Index\",embedding,allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt=\"Give me the names of all case studies in json format?\"\n",
    "most_similar_chunks = db.similarity_search_with_score(user_prompt,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(metadata={'source': 'C:\\\\Users\\\\surya_chakka\\\\BD_Assistant\\\\CaseStudies\\\\Case Study (147).pdf', 'file_path': 'C:\\\\Users\\\\surya_chakka\\\\BD_Assistant\\\\CaseStudies\\\\Case Study (147).pdf', 'page': 0, 'total_pages': 3, 'format': 'PDF 1.7', 'title': 'PowerPoint Presentation', 'author': 'Moumita Gandhi', 'subject': '', 'keywords': '', 'creator': 'Microsoft速 PowerPoint速 for Microsoft 365', 'producer': 'Microsoft速 PowerPoint速 for Microsoft 365', 'creationDate': \"D:20241114131919+05'30'\", 'modDate': \"D:20241114131919+05'30'\", 'trapped': ''}, page_content='Project Name\\nClient\\nBrief Description\\nKey Analyses\\nSpend Cube \\nPrivate Equity\\nConsolidated indirect spend data from multiple data sources and built real time \\ndashboards to provide visibility into indirect spend by spend category, sub-\\ncategory, vendor, customer and across time periods.\\n1\\nSUCCESS STORIES - SUMMARY'),\n",
       " 0.4920848)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Project Name\\nClient\\nBrief Description\\nKey Analyses\\nSpend Cube \\nPrivate Equity\\nConsolidated indirect spend data from multiple data sources and built real time \\ndashboards to provide visibility into indirect spend by spend category, sub-\\ncategory, vendor, customer and across time periods.\\n1\\nSUCCESS STORIES - SUMMARY\\nProject Name\\nBrief Description\\nKey analyses\\nProvided white-label equity research reports by building \\nand analysing bottom-up financial models with all \\nrevenue and cost drivers projected based on \\ncomprehensive understanding of the sector, the company \\nand the macroeconomic factors affecting the trends. \\nAdditionally, ensured turnaround times of about 4-5 weeks \\nfor each stock report.\\n1\\nBI REPORTING CASE STUDY SUMMARY\\n1\\nProject Name\\nClient\\nBrief Description\\nKey Analyses\\nBusiness Performance Reporting for Theatrical \\nProduction company\\nTheatrical Production \\ncompany\\nBuilt executive level performance dashboards and reports at \\nlocation and show level, by aggregating multiple s'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_content = \"\\n\".join([chunk[0].page_content for chunk in most_similar_chunks])\n",
    "aggregated_content[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"case_studies\": [\n",
      "    {\n",
      "      \"Company Profile\": {\n",
      "        \"Company Name\": \"Leading Apparel Brand\",\n",
      "        \"Key Attributes\": {\n",
      "          \"Industry\": \"Apparel and Lifestyle\",\n",
      "          \"Scale\": \"Large, ~100 stores across the U.S.\",\n",
      "          \"Market Segment\": \"Retail and E-commerce\",\n",
      "          \"Key Technologies\": \"Customer Analytics, POS Data\"\n",
      "        }\n",
      "      },\n",
      "      \"Pain Points\": {\n",
      "        \"Explicit\": [\n",
      "          \"Limited insights into customer journey and buying preferences\",\n",
      "          \"Need for better customer acquisition and retention strategies\"\n",
      "        ],\n",
      "        \"Inferred\": [\n",
      "          \"Potential high value customers not being identified\",\n",
      "          \"Sales performance tracking at various customer journey stages is lacking\"\n",
      "        ]\n",
      "      },\n",
      "      \"Relevant Domains\": [\n",
      "        \"Customer Analytics\",\n",
      "        \"Marketing Strategy\",\n",
      "        \"Sales Performance\"\n",
      "      ],\n",
      "      \"Relevant Solutions\": [\n",
      "        \"Leveraged customer level POS data for insights\",\n",
      "        \"Identified key attributes of high value customers\",\n",
      "        \"Tracked sales performance across customer journey stages\",\n",
      "        \"Performed customer sentiment analysis\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"Company Profile\": {\n",
      "        \"Company Name\": \"Apparel Brand\",\n",
      "        \"Key Attributes\": {\n",
      "          \"Industry\": \"Apparel\",\n",
      "          \"Scale\": \"Large, multiple departments\",\n",
      "          \"Market Segment\": \"Retail\",\n",
      "          \"Key Technologies\": \"Promotion Effectiveness Analysis\"\n",
      "        }\n",
      "      },\n",
      "      \"Pain Points\": {\n",
      "        \"Explicit\": [\n",
      "          \"High discounts leading to lower gross profits\",\n",
      "          \"Limited visibility into promotion effectiveness\"\n",
      "        ],\n",
      "        \"Inferred\": [\n",
      "          \"Potential for improved promotion strategy\",\n",
      "          \"Need for systematic tracking of promotions\"\n",
      "        ]\n",
      "      },\n",
      "      \"Relevant Domains\": [\n",
      "        \"Finance\",\n",
      "        \"Operations\",\n",
      "        \"Sales\"\n",
      "      ],\n",
      "      \"Relevant Solutions\": [\n",
      "        \"Evaluated promotions strategy at department and channel level\",\n",
      "        \"Recommended optimal promotion calendar\",\n",
      "        \"Aggregated sales transaction data for analysis\",\n",
      "        \"Identified systematic ways to track promotions\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"Company Profile\": {\n",
      "        \"Company Name\": \"Premium Jewelry Brand\",\n",
      "        \"Key Attributes\": {\n",
      "          \"Industry\": \"Jewelry\",\n",
      "          \"Scale\": \"Premium market segment\",\n",
      "          \"Market Segment\": \"Retail\",\n",
      "          \"Key Technologies\": \"Revenue Forecasting Model\"\n",
      "        }\n",
      "      },\n",
      "      \"Pain Points\": {\n",
      "        \"Explicit\": [\n",
      "          \"Limited visibility into projected business performance\",\n",
      "          \"Need for better marketing strategy development\"\n",
      "        ],\n",
      "        \"Inferred\": [\n",
      "          \"Challenges in managing revenue trends\",\n",
      "          \"Difficulty in assessing impact of promotional activities\"\n",
      "        ]\n",
      "      },\n",
      "      \"Relevant Domains\": [\n",
      "        \"Revenue Management\",\n",
      "        \"Marketing Strategy\"\n",
      "      ],\n",
      "      \"Relevant Solutions\": [\n",
      "        \"Developed customer-based revenue forecasting model\",\n",
      "        \"Provided scenario modeling levers for revenue impact assessment\",\n",
      "        \"Enabled shift from high discount to low discount strategy\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"Company Profile\": {\n",
      "        \"Company Name\": \"Consumer-Focused PE Firm\",\n",
      "        \"Key Attributes\": {\n",
      "          \"Industry\": \"Private Equity\",\n",
      "          \"Scale\": \"Focused on consumer brands\",\n",
      "          \"Market Segment\": \"Investment\",\n",
      "          \"Key Technologies\": \"Due Diligence Analysis\"\n",
      "        }\n",
      "      },\n",
      "      \"Pain Points\": {\n",
      "        \"Explicit\": [\n",
      "          \"Need to evaluate sustainability of growth in target firms\",\n",
      "          \"Challenges in understanding customer behavior\"\n",
      "        ],\n",
      "        \"Inferred\": [\n",
      "          \"Potential for improved investment decisions\",\n",
      "          \"Need for insights on historical growth drivers\"\n",
      "        ]\n",
      "      },\n",
      "      \"Relevant Domains\": [\n",
      "        \"Due Diligence\",\n",
      "        \"Investment Analysis\"\n",
      "      ],\n",
      "      \"Relevant Solutions\": [\n",
      "        \"Analyzed historical growth drivers\",\n",
      "        \"Performed cohort analysis on customer behavior\",\n",
      "        \"Identified key initiatives for top-line and bottom-line growth\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"total_count\": \"4\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "# Define the response schema\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"case_studies\", description=\"List of case studies with their details\"),\n",
    "    ResponseSchema(name=\"total_count\", description=\"Total number of case studies analyzed\")\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# Create prompt template\n",
    "template = \"\"\"Analyze the 'Situation', 'Value Addition', 'Impact' sections from the second page of every case study or pdf and retrieve the case studies that are relevant to the text provided and format the response as a JSON.\n",
    "Extract the case study number and name along with the following details:\n",
    "1. Company Profile: Get company name and summarize the company's key attributes (e.g., industry, scale, market segment, key technologies).\n",
    "2. Pain Points: Highlight the challenges or issues the company is facing. Include explicit and inferred points seperately.\n",
    "3. Relevant Domains: Specify the functional areas or domains these challenges belong to.\n",
    "4. Relevant Solutions: Summarize the solutions that are provided for the pain points in the following pages.\n",
    "\n",
    "Respond in JSON format with keys: \"Company Profile\", \"Pain Points\", \"Relevant Domains\", \"Relevant Solutions\".\n",
    "The response should only be json format that I can directly convert to json file from string.\n",
    "\n",
    "Text to analyze: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Return only the JSON response without any additional text.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Initialize ChatOpenAI\n",
    "llm = ChatOpenAI(api_key=openai_key, model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Get similar chunks and combine them\n",
    "user_question = \"We have a apperal and lifestyle brand client facing continuous average revenue per customer reduction and seeking help with financial planning and anlysis along with the identification of reasons behind reduction \"\n",
    "similar_chunks = db.similarity_search_with_score(user_question, k=10)\n",
    "context = \"\\n\".join([chunk[0].page_content for chunk in similar_chunks])\n",
    "\n",
    "# Format the prompt\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "messages = prompt.format_messages(text=context, format_instructions=format_instructions)\n",
    "\n",
    "# Get response\n",
    "response = llm.invoke(messages)\n",
    "structured_output = output_parser.parse(response.content)\n",
    "\n",
    "# Print formatted JSON\n",
    "import json\n",
    "print(json.dumps(structured_output, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
