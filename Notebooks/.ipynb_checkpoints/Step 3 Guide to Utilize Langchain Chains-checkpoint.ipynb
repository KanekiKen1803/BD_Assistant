{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b86fcf06",
   "metadata": {},
   "source": [
    "### Guide on variety of langchain chain availables\n",
    "\n",
    "#### This notebook provides the content on pre-build chains availables from langchain module.\n",
    "1. retrieval chain\n",
    "2. structure output generation chain\n",
    "3. sql query generation chain\n",
    "      \n",
    "\n",
    "#### Link for further details: https://python.langchain.com/v0.1/docs/modules/chains/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3884bb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chintan_patel\\AppData\\Local\\Temp\\ipykernel_9436\\3695893466.py:16: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding = OpenAIEmbeddings(api_key=openai_key)\n"
     ]
    }
   ],
   "source": [
    "### Initiate LLM object using langchain. Have separate guide.\n",
    "# from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "# from langchain_openai import AzureOpenAIEmbeddings\n",
    "# from langchain_community.chat_models import AzureChatOpenAI\n",
    "\n",
    "## Create embedding of the chunks using Faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# Required following parameters. Update following input parameters. Make sure embedding model and LLM models are deployed within same resource.\n",
    "openai_key=\"<Enter OpenAI Key Here>\"\n",
    "llm = ChatOpenAI(api_key=openai_key,model=\"gpt-4o-mini\")\n",
    "\n",
    "embedding = OpenAIEmbeddings(api_key=openai_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1bc380",
   "metadata": {},
   "source": [
    "#### Chain 1 RetrievalQA : Automate Task of Question and Answer based on Retrieval using RetrievalQA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8555ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DB which created during step 2. Stored to Index folder. Make sure Index Folder is available.\n",
    "try:\n",
    "    db=FAISS.load_local(\"./Index\",embedding)    \n",
    "except:\n",
    "    db=FAISS.load_local(\"./Index\",embedding,allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56aa85a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "retriever = VectorStoreRetriever(vectorstore=db)\n",
    "retrievalQA = RetrievalQA.from_llm(llm=llm, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "194e1cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrievalQA.invoke(\"What is total assests as of december, 31 2023 for Tanico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5e3cf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer>>> {'query': 'What is total assests as of december, 31 2023 for Tanico', 'result': 'The total assets for Tanico Inc. as of December 31, 2023, are $4,970.'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Answer>>>\",response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3fc9f4",
   "metadata": {},
   "source": [
    "####  Chain 2 LLMMathChain : Used to evaluate the mathematical expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d46527a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In back end it uses the prompts and tools to generate answer.Details can be found to the documentation link attached at the start of the session\n",
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "llm_math = LLMMathChain.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c1d1635",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_math.invoke(\"What is result of 20 multiply by 39\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5054d5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is result of 20 multiply by 39', 'answer': 'Answer: 780'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5433f75b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bbc32d-8e16-4cf9-9a01-a0ad5c2afe54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccf5339-6c61-470a-80c9-c2bf047299e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
