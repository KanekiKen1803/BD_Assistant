{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d36fda2",
   "metadata": {},
   "source": [
    "### Guide to Calling LLM APIs\n",
    "\n",
    "#### This notebook provides a step-by-step guide to using various approaches to call Large Language Model (LLM) APIs. Specifically, it demonstrates how to leverage LangChain modules for streamlined integration with LLM APIs. In this guide, you'll find examples for:\n",
    "1. Connecting to OpenAI models\n",
    "2. Integrating with Azure OpenAI models\n",
    "\n",
    "#### Note: OpenAI key will be shared for hackathon with access to the gpt-40-mini model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2a904f",
   "metadata": {},
   "source": [
    "#### Install langchain module\n",
    "1. pip install langchain\n",
    "2. pip install langchain-core\n",
    "3. pip install langchain-community\n",
    "4. pip install langchain-experimental\n",
    "5. pip install langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c54d63",
   "metadata": {},
   "source": [
    "#### Call Openai API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7e37f42",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Call openai api using langchain\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#Add api key. Following object (llm) can be passed as input to most of langchain chain.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m openai_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<Enter OpenAI API Key Here>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_openai'"
     ]
    }
   ],
   "source": [
    "# Call openai api using langchain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "#Add api key. Following object (llm) can be passed as input to most of langchain chain.\n",
    "openai_key=\"<Enter OpenAI API Key Here>\"\n",
    "\n",
    "## Here openai key is passed as client. However, it is not best practice to hardcode the key to the code. \n",
    "## Best pratice is to set up key as enviornemnt variable OPENAI_API_KEY \n",
    "## Environment key OPENAI_API_KEY must be set up if not passed to client.\n",
    "os.environ[\"OPENAI_API_KEY\"]=openai_key\n",
    "\n",
    "llm = ChatOpenAI(api_key=openai_key,model=\"gpt-4o-mini\")\n",
    "\n",
    "response = llm.invoke(\"Hi?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d451577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 9, 'total_tokens': 18, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, id='run-b2ab97d9-297a-43bb-95d2-4c727e584a1b-0', usage_metadata={'input_tokens': 9, 'output_tokens': 9, 'total_tokens': 18, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226115a8",
   "metadata": {},
   "source": [
    "#### Call Azure OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33da3a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import Azure OpenAI\n",
    "# from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# # Required following parameters. Update following input parameters\n",
    "# api_type=\"azure\"\n",
    "# api_key=\"<Add your Azure OpenAI Key>\"\n",
    "# api_version=\"<Version of the API>\"\n",
    "# base_url=\"<API EndPoint of Azure OpenAI>\"\n",
    "# api_deployment=\"<Deployement Name of the Model>\"\n",
    "# model_name=\"<Name of the Model>\"\n",
    "\n",
    "# llm = AzureChatOpenAI(azure_deployment=api_deployment,\n",
    "#                       api_version=api_version,\n",
    "#                       azure_endpoint=base_url,\n",
    "#                       api_key=api_key,\n",
    "#                       model=model_name)\n",
    "\n",
    "# # Run the LLM\n",
    "# response = llm.invoke(\"Tell me a joke\")\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da9dfb",
   "metadata": {},
   "source": [
    "#### Additional Content:\n",
    "Refer to link to explore all LLM service providers integration with langchain. Link: https://python.langchain.com/api_reference/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d31733",
   "metadata": {},
   "source": [
    "#### How to add System Instructions or Customize Prompt with LLM\n",
    "\n",
    "#####  Find all the ways to create prompt template at link: https://python.langchain.com/api_reference/core/prompts.html\n",
    "##### Short Course on Prompt Engineering: https://learn.deeplearning.ai/courses/chatgpt-prompt-eng/lesson/1/introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3c1a47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are multiple ways to add prompt (i.e. system prompt or chat history) while calling the LLMs using langchain. \n",
    "# Here provided most commonally used process.\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Pass variable inside the prompt using the \"{}\". If required to pass curly breackets as default than use double {{}} breackets to deliminate user input.\n",
    "template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is Carl.\"),\n",
    "    (\"human\", \"What is your name?\"),\n",
    "    (\"ai\", \"My name is Carl\"),\n",
    "    (\"human\", \"{input_1}\")\n",
    "])\n",
    "\n",
    "# All variables must added to while invoking the prompt \n",
    "prompt_value = template.invoke({\"input_1\": \"Tell me joke with your name\"})\n",
    "messages = prompt_value.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a84cb066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here you go:\n",
      "\n",
      "Why did Carl bring a ladder to the bar?\n",
      "\n",
      "Because he heard the drinks were on the house!\n"
     ]
    }
   ],
   "source": [
    "# LLM is object of langchain\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02cd8ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b53bbb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edbc9878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sure! Here you go:\\n\\nWhy did Carl bring a ladder to the bar?\\n\\nBecause he heard the drinks were on the house!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 46, 'total_tokens': 72, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, id='run-ecb2fe5f-c918-4a9b-a670-568d15371793-0', usage_metadata={'input_tokens': 46, 'output_tokens': 26, 'total_tokens': 72, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfd776b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
