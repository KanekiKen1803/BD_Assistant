{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d36fda2",
   "metadata": {},
   "source": [
    "### Guide to Calling LLM APIs\n",
    "\n",
    "#### This notebook provides a step-by-step guide to using various approaches to call Large Language Model (LLM) APIs. Specifically, it demonstrates how to leverage LangChain modules for streamlined integration with LLM APIs. In this guide, you'll find examples for:\n",
    "1. Connecting to OpenAI models\n",
    "2. Integrating with Azure OpenAI models\n",
    "\n",
    "#### Note: OpenAI key will be shared for hackathon with access to the gpt-40-mini model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2a904f",
   "metadata": {},
   "source": [
    "#### Install langchain module\n",
    "1. pip install langchain\n",
    "2. pip install langchain-core\n",
    "3. pip install langchain-community\n",
    "4. pip install langchain-experimental\n",
    "5. pip install langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c54d63",
   "metadata": {},
   "source": [
    "#### Call Openai API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7e37f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Call openai api using langchain\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "openai_key = ''\n",
    "#Add api key. Following object (llm) can be passed as input to most of langchain chain.\n",
    "## Here openai key is passed as client. However, it is not best practice to hardcode the key to the code. \n",
    "## Best pratice is to set up key as enviornemnt variable OPENAI_API_KEY \n",
    "## Environment key OPENAI_API_KEY must be set up if not passed to client.\n",
    "openai_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(api_key=openai_key,model=\"gpt-4o-mini\")\n",
    "\n",
    "response = llm.invoke(\"Hi?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d451577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 9, 'total_tokens': 18, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, id='run-a4da89b0-664b-4bd4-ac02-2e2c0fbe2550-0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226115a8",
   "metadata": {},
   "source": [
    "#### Call Azure OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33da3a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import Azure OpenAI\n",
    "# from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# # Required following parameters. Update following input parameters\n",
    "# api_type=\"azure\"\n",
    "# api_key=\"<Add your Azure OpenAI Key>\"\n",
    "# api_version=\"<Version of the API>\"\n",
    "# base_url=\"<API EndPoint of Azure OpenAI>\"\n",
    "# api_deployment=\"<Deployement Name of the Model>\"\n",
    "# model_name=\"<Name of the Model>\"\n",
    "\n",
    "# llm = AzureChatOpenAI(azure_deployment=api_deployment,\n",
    "#                       api_version=api_version,\n",
    "#                       azure_endpoint=base_url,\n",
    "#                       api_key=api_key,\n",
    "#                       model=model_name)\n",
    "\n",
    "# # Run the LLM\n",
    "# response = llm.invoke(\"Tell me a joke\")\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da9dfb",
   "metadata": {},
   "source": [
    "#### Additional Content:\n",
    "Refer to link to explore all LLM service providers integration with langchain. Link: https://python.langchain.com/api_reference/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d31733",
   "metadata": {},
   "source": [
    "#### How to add System Instructions or Customize Prompt with LLM\n",
    "\n",
    "#####  Find all the ways to create prompt template at link: https://python.langchain.com/api_reference/core/prompts.html\n",
    "##### Short Course on Prompt Engineering: https://learn.deeplearning.ai/courses/chatgpt-prompt-eng/lesson/1/introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3c1a47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are multiple ways to add prompt (i.e. system prompt or chat history) while calling the LLMs using langchain. \n",
    "# Here provided most commonally used process.\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Pass variable inside the prompt using the \"{}\". If required to pass curly breackets as default than use double {{}} breackets to deliminate user input.\n",
    "template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is Carl.\"),\n",
    "    (\"human\", \"What is your name?\"),\n",
    "    (\"ai\", \"My name is Carl\"),\n",
    "    (\"human\", \"{input_1}\")\n",
    "])\n",
    "\n",
    "# All variables must added to while invoking the prompt \n",
    "prompt_value = template.invoke({\"input_1\": \"Tell me joke with your name\"})\n",
    "messages = prompt_value.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a84cb066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here’s a joke for you:\n",
      "\n",
      "Why did Carl always carry a ladder?\n",
      "\n",
      "Because he wanted to reach new heights in conversation!\n"
     ]
    }
   ],
   "source": [
    "# LLM is object of langchain\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02cd8ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b53bbb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edbc9878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sure! Here’s a joke for you:\\n\\nWhy did Carl always carry a ladder?\\n\\nBecause he wanted to reach new heights in conversation!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 46, 'total_tokens': 73, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_9b78b61c52', 'finish_reason': 'stop', 'logprobs': None}, id='run-ce506b7e-0e9f-4dcd-91e6-6f5cba68794a-0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfd776b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
